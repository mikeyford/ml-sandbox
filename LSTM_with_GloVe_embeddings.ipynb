{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/kixi_messages.csv\")\n",
    "df = df.sample(frac=1)\n",
    "y = df.pop('channel')\n",
    "labels = np.asarray(pd.get_dummies(y))                    \n",
    "# one-hot encoded numpy array for output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some basic parsing on the dataset (this might be more easily doable with a library too)\n",
    "\n",
    "df.text = df.text.str.lower()\n",
    "df.text = df.text.str.replace('\\n', ' ')\n",
    "df.text = df.text.str.replace('[\\Â£]', 'pounds')\n",
    "df.text = df.text.str.replace('[\\_\\.\\,\\_\\-\\!\\?]', ' ')\n",
    "df.text = df.text.str.replace('[^a-z\\s]', '')\n",
    "df.text = df.text.str.replace('\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100                                              \n",
    "# how many words to cut off the messages at\n",
    "\n",
    "validation_split = 0.8                                    \n",
    "# how to split the training and testing data\n",
    "\n",
    "training_samples = round(df.shape[0]*validation_split)    \n",
    "# number of training samples\n",
    "\n",
    "validation_samples = df.shape[0] - training_samples       \n",
    "# number of testing samples\n",
    "\n",
    "max_words = 10000                                         \n",
    "# consider only the 1000 most common words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17297 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)                \n",
    "# instantiate the keras tokenizer which turns each word into an integer representation\n",
    "\n",
    "tokenizer.fit_on_texts(df.text)                           \n",
    "# fit it on our datset\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df.text)         \n",
    "# generate a sequence of integers to represent each input row\n",
    "\n",
    "print(\"Found %s unique tokens.\" % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (9588, 100)\n",
      "Shape of labels: (9588, 5)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)            \n",
    "# if a sequence contains less than 100 words, fill the rest of it with zeros\n",
    "\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training/test\n",
    "\n",
    "X_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "X_test = data[training_samples: training_samples + validation_samples]\n",
    "y_test = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# loading the GloVe word representation vectors (download and info here: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open('/Users/mike/GloVe/common_crawl/glove.42B.300d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300                                        \n",
    "# dimensionality of GloVe vectors used\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))    \n",
    "# instantiate empty matrix for embedding\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector         \n",
    "# if top 10000 dataset word is in GloVe, embed the representation, otherwise leave it zeros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "model = Sequential()                                       \n",
    "# instantiate sequential architecture\n",
    "\n",
    "model.add(Embedding(max_words,                             \n",
    "                    embedding_dim,\n",
    "                    input_length=maxlen))\n",
    "# layer which receives 2D tensor of batchsize (inferred) x max_words, outputs 3D tensor of batchsize x max_words x embedding_dim\n",
    "\n",
    "model.add(Bidirectional(LSTM(128,                           \n",
    "                             return_sequences=True, \n",
    "                             dropout=0.2,\n",
    "                             recurrent_dropout=0.2)))      \n",
    "# bidirectional layer with 128 LSTM units, and dropout to regularize\n",
    "\n",
    "model.add(GlobalMaxPool1D())               \n",
    "# pooling layer to receive a 3D tensor and combine clusters into a 2D output\n",
    "\n",
    "model.add(Dense(128, activation='relu'))                   \n",
    "# standard hidden layer with 128 relu units\n",
    "\n",
    "model.add(Dropout(0.2))                                    \n",
    "# dropout layer for regularisation\n",
    "\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))    \n",
    "# output layer with a softmax unit for each class to output a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])            \n",
    "# load the GloVe embeddings into the first layer of the network\n",
    "\n",
    "model.layers[0].trainable = False                          \n",
    "# freeze the GloVe embeddings to the model can't update them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 100, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 100, 256)          439296    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,472,837\n",
      "Trainable params: 472,837\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', patience=3)]  \n",
    "# stop fitting if validation accuracy goes down for more than 3 epochs in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',                             \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "# compile the model using fairly standard settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7670 samples, validate on 1918 samples\n",
      "Epoch 1/50\n",
      "7670/7670 [==============================] - 46s 6ms/step - loss: 1.3802 - acc: 0.4186 - val_loss: 1.1989 - val_acc: 0.5266\n",
      "Epoch 2/50\n",
      "7670/7670 [==============================] - 46s 6ms/step - loss: 1.1443 - acc: 0.5606 - val_loss: 1.0881 - val_acc: 0.5881\n",
      "Epoch 3/50\n",
      "7670/7670 [==============================] - 45s 6ms/step - loss: 1.0323 - acc: 0.6056 - val_loss: 1.0170 - val_acc: 0.6147\n",
      "Epoch 4/50\n",
      "7670/7670 [==============================] - 45s 6ms/step - loss: 0.9438 - acc: 0.6405 - val_loss: 0.9726 - val_acc: 0.6319\n",
      "Epoch 5/50\n",
      "7670/7670 [==============================] - 45s 6ms/step - loss: 0.8700 - acc: 0.6739 - val_loss: 0.9665 - val_acc: 0.6335\n",
      "Epoch 6/50\n",
      "7670/7670 [==============================] - 44s 6ms/step - loss: 0.8017 - acc: 0.7057 - val_loss: 0.9306 - val_acc: 0.6460\n",
      "Epoch 7/50\n",
      "7670/7670 [==============================] - 46s 6ms/step - loss: 0.7350 - acc: 0.7312 - val_loss: 0.9181 - val_acc: 0.6517\n",
      "Epoch 8/50\n",
      "7670/7670 [==============================] - 44s 6ms/step - loss: 0.6669 - acc: 0.7601 - val_loss: 0.9611 - val_acc: 0.6507\n",
      "Epoch 9/50\n",
      "7670/7670 [==============================] - 45s 6ms/step - loss: 0.6107 - acc: 0.7801 - val_loss: 0.9462 - val_acc: 0.6637\n",
      "Epoch 10/50\n",
      "7670/7670 [==============================] - 45s 6ms/step - loss: 0.5343 - acc: 0.8134 - val_loss: 0.9558 - val_acc: 0.6601\n",
      "Epoch 11/50\n",
      "7670/7670 [==============================] - 45s 6ms/step - loss: 0.4788 - acc: 0.8295 - val_loss: 0.9765 - val_acc: 0.6533\n",
      "Epoch 12/50\n",
      "7670/7670 [==============================] - 45s 6ms/step - loss: 0.4243 - acc: 0.8563 - val_loss: 1.0030 - val_acc: 0.6595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8fd29c50>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,                                          \n",
    "          y_train, \n",
    "          epochs=50, \n",
    "          batch_size=128, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=callbacks)\n",
    "# fit the model using a validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the model produces best test accuracy of over 65% after 7 epochs. (vs. about 20% accuracy from chance)\n",
    "# Pretty good for a relatively simply network and considering the likely crossover in topics between kixi channels!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
